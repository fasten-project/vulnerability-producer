/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package eu.fasten.vulnerabilityproducer.utils.parsers;

import com.mongodb.client.MongoDatabase;
import eu.fasten.vulnerabilityproducer.db.NitriteController;
import eu.fasten.vulnerabilityproducer.utils.PatchFinder;
import eu.fasten.vulnerabilityproducer.utils.Vulnerability;
import eu.fasten.vulnerabilityproducer.utils.connections.JavaHttpClient;
import eu.fasten.vulnerabilityproducer.utils.mappers.PurlMapper;
import eu.fasten.vulnerabilityproducer.utils.mappers.VersionRanger;
import me.tongfei.progressbar.ProgressBar;
import org.apache.commons.io.FileUtils;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.jooq.tools.json.JSONParser;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.*;
import java.util.*;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.*;

/**
 * The responsibility of the ParserManager is to gather Vulnerabilities from all the parsers.
 * Combine them together, merging what is necessary and the handing them over to the PatchFarmer to enrich
 * every single vulnerability with information about patches specifically.
 */
public class ParserManager {
    private ExtraParser extraParser;
    private GHParser ghParser;
    private String ghToken;
    private NVDParser nvdParser;
    private OVALParser ovalParser;
    private PatchFinder patchFinder;
    private VersionRanger versionRanger;
    private NitriteController nitriteController;
    private PurlMapper purlMapper;
    private final Logger logger = LoggerFactory.getLogger(ParserManager.class.getName());
    private String mnt;


    public ParserManager(JavaHttpClient client,
                         MongoDatabase mongoDatabase,
                         NitriteController nitriteController,
                         String ghToken,
                         String mnt) {
        this.mnt = mnt;
        this.ghToken = ghToken;
        this.nitriteController = nitriteController;
        this.versionRanger = new VersionRanger(client, mnt + "/trackers/package_versions.json");
        this.extraParser = new ExtraParser(client, versionRanger, mnt);
        this.ovalParser = new OVALParser(client, versionRanger);
        this.ghParser = new GHParser(client, ghToken, versionRanger, mnt + "/trackers/ghcursor.txt");
        this.patchFinder = new PatchFinder(mongoDatabase, client, ghToken);
        this.purlMapper = new PurlMapper(versionRanger, patchFinder, mnt + "/datasets/purl_maps/");
        this.nvdParser = new NVDParser(new JSONParser(), client, this.purlMapper, mnt);
    }

    /**
     * Combines the results of all the parsers.
     *
     * @return set of vulnerability objects containing all the information found.
     */
    public void getVulnerabilitiesFromParsers(KafkaProducer<String, String> kafkaProducer, String topic) {
        logger.info("Gathering vulnerabilities from GitHub Advisories");
        var mapFromGH = ghParser.getVulnerabilities();
        logger.info("Gathering vulnerabilities from Extra Sources");
        var mapFromExtraSources = extraParser.getVulnerabilities();
        logger.info("Gathering vulnerabilities from NVD");
        var mapFromNVD = nvdParser.getVulnerabilities();
        logger.info("Gathering vulnerabilities from Debian OVAL");
        var mapFromOVAL = ovalParser.getVulnerabilities();

        logger.info("Merging all the pulled vulnerabilities");
        var maps = new ArrayList<HashMap<String, Vulnerability>>();
        maps.add(mapFromNVD);
        maps.add(mapFromExtraSources);
        maps.add(mapFromGH);
        maps.add(mapFromOVAL);
        var vulnerabilities = mergeMapsOfVulnerabilities(maps);

        logger.info("Injecting information about Patches");

        try (ProgressBar pb = new ProgressBar("ParserManager", vulnerabilities.size())) {
            while (!vulnerabilities.isEmpty()) {
                var v = vulnerabilities.poll();
                logger.info("Looking for patches in references of " + v.getId());
                patchFinder.parseReferences(v, nitriteController);
                purlMapper.inferPurls(v);
                if (!isDuplicate(v)) {
                    logger.info("Publishing " + v.getId() + " to Kafka");
                    kafkaProducer.send(new ProducerRecord<>(topic, v.toJson()));
                }

                pb.step();
            }
        }
    }

    public static void storeVulnJson(Vulnerability v) {
        // Storing the file to statements
        File file = new File("/path/to/dir/" + v.getId() + ".json");
        try {
            FileUtils.writeStringToFile(file, v.toJson());
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    /**
     * Get updates from all the different parsers and return them.
     *
     * @return set of vulnerabilities, including new ones and changed ones.
     */
    public void getUpdatesFromParsers(KafkaProducer<String, String> kafkaProducer, String topic) {
        logger.info("Updating Versions of Packages");
        versionRanger.updateMappings();

        logger.info("Gathering updates from GitHub Advisories");
        HashMap<String, Vulnerability> mapFromGH = ghParser.getUpdates();
        logger.info("Gathering updates from NVD");
        HashMap<String, Vulnerability> mapFromNVD = nvdParser.getUpdates();
        logger.info("Gathering updates from OVAL");
        HashMap<String, Vulnerability> mapFromOVAL = ovalParser.getUpdates();

        logger.info("Merging all the pulled updates");
        // Put all maps from parsers into a list to pass it on
        List<HashMap<String, Vulnerability>> maps = new ArrayList<>();
        maps.add(mapFromNVD);
        maps.add(mapFromGH);
        maps.add(mapFromOVAL);
        var vulnerabilities = mergeMapsOfVulnerabilities(maps);

        logger.info("Parsing references of all vulnerabilities to inject patches information");
        while (!vulnerabilities.isEmpty()) {
            var v = vulnerabilities.poll();
            // Adding patch information
            patchFinder.parseReferences(v, nitriteController);
            // Infer purls
            purlMapper.inferPurls(v);
            // Store statement
            // storeVulnJson(v);
            // Publish to kafka if new
            if (!isDuplicate(v)) {
                logger.info("Publishing " + v.getId() + " to Kafka");
                kafkaProducer.send(new ProducerRecord<>(topic, v.toJson()));
            }
        }
    }

    /**
     * Looks in Nitrite storage to find if duplicate.
     * @param v - Vulnerability object
     * @return boolean
     */
    public boolean isDuplicate(Vulnerability v) {
        var queryFromNC = nitriteController.findVulnerabilityEntry(v.getId());
        if (queryFromNC.isPresent()) {
            if (queryFromNC.get().equals(v)) {
                return true;
            }
        }
        return false;
    }

    /**
     * Given a list of vulnerability objects, it merges all of the objects.
     * @param maps - List of hashmaps mapping ID to Vulnerability objects
     * @return set of vulnerabilities
     */
    private PriorityQueue<Vulnerability> mergeMapsOfVulnerabilities(List<HashMap<String, Vulnerability>> maps) {
        var mapMerger = new HashMap<String, Vulnerability>();
        maps.forEach(map -> map.keySet().forEach(vId -> {
            if (mapMerger.containsKey(vId)) {
                var v = mapMerger.get(vId);
                v.merge(map.get(vId));
                mapMerger.put(vId, v);
            } else {
                mapMerger.put(vId, map.get(vId));
            }
        }));
        return new PriorityQueue<>(mapMerger.values());
    }


    /**
     * Helper function for the vulnerability Plugin to inject the sleep behaviour.
     * This way, when mocking the parser, you can just skip over the wait.
     */
    public void sleep() {
        try {
            TimeUnit.DAYS.sleep(1);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }

    /**
     * Helper method to inject parsers and test the merging of the three sources.
     *
     * @param nvdParser   - NVDParser
     * @param ghParser    - GHParser
     * @param extraParser - ExtraParser
     * @param patchFinder - PatchFarmer
     */
    public void injectParsers(NVDParser nvdParser,
                              GHParser ghParser,
                              ExtraParser extraParser,
                              OVALParser ovalParser,
                              PatchFinder patchFinder,
                              PurlMapper purlMapper) {
        this.nvdParser = nvdParser;
        this.ghParser = ghParser;
        this.extraParser = extraParser;
        this.ovalParser = ovalParser;
        this.patchFinder = patchFinder;
        this.purlMapper = purlMapper;
    }

    /**
     * Method to evaluate the crawler based on SAP MSR2019 data.
     * Check https://github.com/fasten-project/vulnerability-producer/issues/57 for more info.
     */
    public void evaluateCrawler() {
        var gt = extraParser.getGroundThruth();
        var patches = new HashMap<String, HashSet<String>>();

        var nvd = nvdParser.getVulnerabilities();

        try (var pb = new ProgressBar("Crawling", gt.size())) {
            nvd.keySet().stream().filter(gt::containsKey).forEach(id -> {
                var v = nvd.get(id);
                patchFinder.parseReferences(v, nitriteController);
                patches.put(id, v.getPatchLinks());
            });
            pb.step();
        }

        var counter = new AtomicInteger();
        var notFound = new HashSet<String>();

        // TODO: Diff comparison
        gt.keySet().forEach(id -> {
            var gtPatches = gt.get(id);
            var foundPatches = patches.get(id);
            gtPatches.forEach(gtp -> {
                if (foundPatches.contains(gtp)) {
                    counter.addAndGet(1);
                } else {
                    notFound.add(id);
                }
            });
        });

        System.out.println("Found a patch for " + counter.get() + " vulnerabilities out of " + gt.size());
        System.out.println("Success rate: " + (float) (counter.get() / gt.size()));
    }
}
